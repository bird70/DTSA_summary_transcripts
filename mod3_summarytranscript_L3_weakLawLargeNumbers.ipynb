{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "# Mod3/L3 Weak Law of Large Numbers and Convergence of Random Variables\n",
    "\n",
    "## Introduction\n",
    "In this lesson, we discuss the concept of convergence of sequences of random variables. This is important because our estimators are random variables that depend on the sample size $(n)$. We want to understand how these estimators behave as the sample size increases.\n",
    "\n",
    "## Convergence of Sequences of Random Variables\n",
    "### Types of Convergence\n",
    "1. **Asymptotic Unbiasedness**: The expected value of the estimator converges to the parameter as the sample size increases.\n",
    "2. **Convergence in Probability**: For any $(\\epsilon > 0)$, a sequence of random variables $(X_n)$ converges in probability to $(X)$ if:\n",
    "   $[ \\lim_{n \\to \\infty} P(|X_n - X| \\geq \\epsilon) = 0 ]$\n",
    "\n",
    "### Chebyshev's Inequality\n",
    "Chebyshev's inequality is a key tool for proving convergence in probability. It states that for a random variable $(X)$ with mean $(\\mu)$ and variance $(\\sigma^2)$:\n",
    "$[ P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2} ]$\n",
    "\n",
    "### Proof of Chebyshev's Inequality\n",
    "1. **Markov's Inequality**: For a non-negative function $(g(X))$ and $(c > 0)$:\n",
    "   $[ P(g(X) \\geq c) \\leq \\frac{E[g(X)]}{c} ]$\n",
    "2. **Chebyshev's Inequality**: Applying Markov's inequality to $(g(X) = (X - \\mu)^2)$ and $(c = (k\\sigma)^2)$:\n",
    "   $[ P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{\\sigma^2}{k^2\\sigma^2} = \\frac{1}{k^2} ]$\n",
    "\n",
    "## Weak Law of Large Numbers (WLLN)\n",
    "The WLLN states that if $(X_1, X_2, \\ldots, X_n)$ are iid random variables with mean $(\\mu)$ and finite variance $(\\sigma^2)$, then the sample mean $(\\bar{X}_n)$ converges in probability to $(\\mu)$:\n",
    "$[ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i ]$\n",
    "$[ \\lim_{n \\to \\infty} P(|\\bar{X}_n - \\mu| \\geq \\epsilon) = 0 ]$\n",
    "\n",
    "### Proof of WLLN\n",
    "1. Fix $(\\epsilon > 0)$.\n",
    "2. Apply Chebyshev's inequality to the sample mean:\n",
    "   $[ P(|\\bar{X}_n - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{n\\epsilon^2} ]$\n",
    "3. As $(n \\to \\infty)$, the right-hand side approaches 0, proving convergence in probability.\n",
    "\n",
    "### Example Applications\n",
    "1. **Exponential Distribution**: For a random sample from an exponential distribution with rate $(\\lambda)$, the sample mean $(\\bar{X}_n)$ converges in probability to $(1/\\lambda)$.\n",
    "2. **Gamma Distribution**: For a random sample from a gamma distribution with parameters $(\\alpha)$ and $(\\beta)$, the sample mean $(\\bar{X}_n)$ converges in probability to $(\\alpha/\\beta)$.\n",
    "\n",
    "## Conclusion\n",
    "The weak law of large numbers (WLLN) and the concept of convergence in probability are crucial for understanding the behavior of estimators as the sample size increases. These concepts ensure that our estimators become more accurate with larger samples.\n",
    "\n",
    "This concludes the lesson on the weak law of large numbers and convergence of random variables. In the next lessons (refer to [mod3_summarytranscript_L4_ConvergenceInDistrib_and_NormalDist.ipynb](mod3_summarytranscript_L4_ConvergenceInDistrib_and_NormalDist.ipynb)), we will continue to explore more advanced topics and applications in statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
